---
layout: post
title: LDA(主题模型)
categories: ML 
---

LDA(Latent Dirichlet Allocation)是一种**文档主题生成模型**，也称为一个**三层贝叶斯概率模型**，包含**词、主题和文档三层结构**。所谓生成模型由如下假设而来：一篇文章的每个词都是通过这样的过程生成而来：**先按某种概率分布选择某个主题，再从该主题中按某种概率分布选择某个词。** 文档到主题服从多项式分布，主题到词服从多项式分布。

LDA是一种**非监督式**机器学习技术，可用来识别大规模文档集或语料库中**潜在的主题信息**。其采用**词袋**的方法，将每一篇文档看成一个词频向量，从而将文本信息转化为数字信息。**每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。**


## LDA生成过程
对于语料库中的每篇文档，其生成过程如下：

- (1) 从**文档-主题分布**中采样一个主题
- (2) 从被采样到的主题所对应的**单词分布**中采样一个单词
- (3) 重复(1)、(2)，直至遍历文档中的每一个单词

其中每篇文档与$$T$$(超参数)个主题的多项分布相对应，记为$$\theta$$，即**文档-主题分布**。每个主题与词表中$$V$$个单词的一个多项分布相对应，记为$$\varphi$$，即**主题-单词分布**。

**文档-主题分布**与**主题-单词分布**就是LDA模型待学习的**模型参数。**


## LDA整体流程
令文档集合为$$D$$，主题集合为$$T$$。$$D$$中每个文档$$d$$看作一个单词序列$$\left\{ w_1,w_2,\cdots ,w_n \right\}$$ (**该单词序列不考虑词序，即词袋**)，$$D$$中的所有单词组成一个大集合，记为$$VOC$$。

LDA以文档集合$$D$$为**输入**，训练出如下两个结果向量(设共有$$k$$个主题(topic)，$$VOC$$中共有$$m$$个词)作为**输出**：

- 对$$D$$中的每个文档$$d$$，对应到不同主题(topic)的概率分布为$$
\theta _d\left( p_{t_1},\cdots ,p_{t_k} \right) 
$$，其中$$p_{t_i}$$表示$$d$$对应第$$i$$个topic的概率，且$$
p_{t_i}=\dfrac{n_{t_i}}{n}
$$。其中$$n_{t_i}$$表示文档$$d$$中属于第$$i$$个topic的词的数目，$$n$$表示文档$$d$$中所有词的总数。

- 对$$T$$中的每个topic，生成不同单词的概率分布为$$
\varphi _t\left( p_{w_1},\cdots ,p_{w_m} \right) 
$$，其中$$p_{w_i}$$表示主题$$t$$生成$$VOC$$中第$$i$$个单词的概率，且$$
p_{w_i}=\dfrac{N_{w_i}}{N}
$$。其中$$N_{w_i}$$表示对应到主题$$t$$中第$$i$$个单词的数目，$$N$$表示对应到主题$$t$$的所有单词的个数。

由上述生成的两组概率，可反向推出文档$$d$$的生成过程：

$$
p\left( w\left| d \right. \right) =p\left( t\left| d \right. \right) *p\left( w\left| t \right. \right) 
$$

其中$$p\left( t\left\| d \right. \right)$$由$$
\theta _d\left( p_{t_1},\cdots ,p_{t_k} \right) 
$$可得；$$p\left( w\left\| t \right. \right)$$由$$
\varphi _t\left( p_{w_1},\cdots ,p_{w_m} \right) 
$$可得。以主题$$t$$为中间层，利用$$\theta _d$$和$$\varphi _t$$可计算文档$$d$$中出现单词$$w$$的概率。



## 参考

- [用LDA处理文本(Python)](https://blog.csdn.net/u013710265/article/details/73480332)

- [LDA数学八卦索引及全文文档](https://zhuanlan.zhihu.com/p/57418059)
