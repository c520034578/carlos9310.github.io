---
layout: post
title: 机器阅读理解中数据增强笔记
categories: [NLP] 
---

## 背景
之前梳理过关于机器阅读理解的[相关内容](https://carlos9310.github.io/2019/09/27/MRC-squad2.0/)，这里主要记录如何通过数据增强的trick来进一步提升性能指标。

当答案为文档中的一个连续片段(**可看成一种实体**)时，系统已可十分准确地从文档中抽取答案。但当给定文档中无法找到某个问题的答案时，系统应拒绝回答。而这类无法回答的问题的难度相比可抽取片段(实体)的问题要大，且数量较少。

针对上述问题，[Learning to Ask Unanswerable Questions for Machine Reading Comprehension](https://arxiv.org/abs/1906.06045)中提出seq2seq与pair2seq的模型，利用可回答问题与相应段落来生成不可回答问题。实验表明，增强后的数据可有效提升系统的性能。
 
## 数据观察
SQuAD 2.0数据集中新增的无法回答问题，有一个似是而非(plausible)的答案；而这个似是而非的答案也是某个可回答问题的答案。即两种问题对应同一个(实体)片段。示意图如下：
 
![png](/assets/images/nlp/mrc/mrc-da-01.png)

基于上述观察，对可回答问题进行适当修改就能得到相应的不可回答问题。 

## 问题描述
给定可回答的问题$$q$$和包含答案$$a$$的段落$$p$$，生成不可回答的问题$$\tilde{q}$$。**生成的不可回答问题需满足如下要求**
- 问题$$\tilde{q}$$在段落$$p$$中找不到答案
- 问题$$\tilde{q}$$需要与可回答的问题$$q$$和段落$$p$$都相关，这可避免产生不相关的问题
- 问题$$\tilde{q}$$需问一些和答案$$a$$相关的问题。

基于上述要求，Learning to Ask Unanswerable Questions for Machine Reading Comprehension中研究了两种基于编码器-解码器结构的神经网络模型(用于生成不可回答的问题)。示意图如下：

![png](/assets/images/nlp/mrc/mrc-da-02.png)

其中seq2seq模型将拼接后的段落与问题作为输入，并以序列的方式进行编码。而pair2seq模型先对段落和问题分别编码，然后基于**注意力机制**捕获问题和段落间的交互，得到**基于问题的段落表示和基于段落的问题表示**，两种表示共同用于解码。(为了能够更有效地利用输入的问题和段落来生成无法回答的问题，在解码时还采用**复制机制**，将输入序列中的token复制到输出中。)

上述两种模型的解码器依次解码出无法回答的问题，其生成概率为$$P\left( \tilde{q}\left\| q,p,a \right. \right) $$。由语言模型可将此概率分解为：

$$
P\left( \tilde{q}\left| q,p,a \right. \right) =\prod_{t=1}^{\left| \tilde{q} \right|}{P\left( \tilde{q}_t\left| \tilde{q}_{<t},q,p,a \right. \right)}
$$

其中$$\tilde{q}_{<t}=\tilde{q}_1,\tilde{q}_2,\cdots ,\tilde{q}_{t-1}$$

### seq2seq
**seq2seq模型的输入序列$$x$$的embedding层由word embeddings、character embeddings和token type embeddings三部分组成，三部分的和为embedding层的输出，且第$$i$$个token对应的embedding记为$$e_i$$。** 其中token type是为了区分段落与问题，有answer(A)、paragraph(P)和question(Q)三种类型；character embeddings是通过对word embeddings进行最大池化操作所得。

**encoder层**为单层双向的LSTM，其隐状态$$h_i=f_{BiLSTM}\left( h_{i-1},e_i \right) $$。**decoder层**为单层单向的LSTM，其隐状态$$s_t=f_{LSTM}\left( s_{t-1},\left[ y_{t-1};c_{t-1} \right] \right) $$。其中$$y_{t-1}$$表示$$t-1$$时刻对应的解码输出，$$c_{t-1}$$表示$$t-1$$时刻解码器对应的语义编码，该**语义编码是基于attention机制对编码器中的隐状态求和得到的**。关于语义编码的相关计算如下所示：

$$
c_t=\sum_i^{\left| x \right|}{\alpha _{t,i}h_i}
$$

其中$$\alpha _{t,i}=\exp \left( score\left( h_i,s_t \right) \right) /Z_t$$表示隐状态$$h_i$$的权重，$$score\left( h_i,s_t \right) =h_{i}^{T}W_{\alpha}s_t$$表示$$h_i$$与$$s_t$$相关性得分，$$Z_t=\sum_k^{\left\| x \right\|}{\exp \left( score\left( h_k,s_t \right) \right)}$$，$$W_{\alpha}$$为可学习的参数。

图示说明如下：

![png](/assets/images/nlp/mrc/mrc-da-03.png)

基于上述的$$s_t$$与$$c_t$$可确定生成每个词汇对应的概率：

$$
P_v=softmax \left( W_v\left[ s_t;c_t \right] +b_v \right) 
$$

其中$$W_v$$与$$b_v$$为可学习的参数。

由于段落中的词或可回答问题中的词对生成无法回答的问题很有参考价值，因此引入**复制机制**，直接从输入部分复制词。具体地，通过$$s_t$$与$$c_t$$生成一个门控概率$$g_t$$：

$$
g_t=sigmoid\left( W_g\left[ s_t;c_t \right] +b_g \right) 
$$

其中$$W_g$$与$$b_g$$为可学习的参数。$$g_t$$决定了生成的词是从词表中选的还是从输入部分复制的。

最终得到生成词$$\tilde{q}_t$$的概率为：

$$
P\left( \tilde{q}_t\left\| \tilde{q}_{<t},q,p,a \right. \right) =g_tP_v\left( \tilde{q}_t \right) +\left( 1-g_t \right) \sum_{i\in \zeta _{\tilde{q}_t}}{\hat{\alpha}_{i,t}}
$$

其中$$\zeta _{\tilde{q}_t}$$表示词$$\tilde{q}_t$$在输入序列中出现的所有位置索引的集合。$$\hat{\alpha}_{i,t}$$的计算方式与$$\alpha_{i,t}$$相同但参数不同。

### pair2seq
段落与问题的交互在机器阅读理解中起着至关重要的作用。交互使段落和问题相互意识到彼此，从而有助于更准确地预测答案。基于上述事实，提出了一种pair2seq的模型。

pair2seq模型的**embedding层**将段落和问题分别编码为$$e_{i}^{p}$$和$$e_{j}^{q}$$，**encoder层**是两个共享权重的单层双向LSTM，基于embedding层的输入可分别得到**段的隐状态**$$h_{i}^{p}=f_{BiLSTM}\left( h_{i-1}^{p},e_{i}^{p} \right)$$和**问题的隐状态**$$h_{j}^{q}=f_{BiLSTM}\left( h_{j-1}^{q},e_{j}^{q} \right) $$。**interaction层**使用和seq2seq模型中同样的attention机制分别生成**意识到问题的段落表示**$$\tilde{h}_{i}^{p}$$和**意识到段落的问题表示**$$\tilde{h}_{j}^{q}$$。

关于$$\tilde{h}_{i}^{p}$$的具体公式为：

$$
\tilde{h}_{i}^{p}=\tan\text{h}\left( W_p\left[ h_{i}^{p};\hat{h}_{i}^{p} \right] +b_p \right) 
$$

其中$$\hat{h}_{i}^{p}=\sum_{j=1}^{\left\| q \right\|}{\alpha _{i,j}h_{j}^{q}}$$表示问题序列中$$q$$个token相对于段落中的第$$i$$个token的语义表示(类似于seq2seq中的$$c_t$$)。$$\alpha _{i,j}=\exp \left( score\left( h_{i}^{p},h_{j}^{q} \right) \right) /Z_i$$表示问题序列中每个token对应的权重。$$Z_i=\sum_{k=1}^{\left\| q \right\|}{\exp \left( score\left( h_{i}^{p},h_{k}^{q} \right) \right)}$$。$$W_p$$与$$b_p$$为可学习的参数。

相似地，关于$$\tilde{h}_{j}^{q}$$的具体公式为：

$$
\tilde{h}_{j}^{q}=\tan\text{h}\left( W_q\left[ h_{j}^{q};\hat{h}_{j}^{q} \right] +b_q \right) 
$$

其中$$\hat{h}_{j}^{q}=\sum_{i=1}^{\left\| p \right\|}{\beta _{i,j}h_{i}^{p}}$$表示段落序列中$$p$$个token相对于问题中的第$$j$$个token的语义表示(类似于seq2seq中的$$c_t$$)。$$\beta _{i,j}=\exp \left( score\left( h_{i}^{p},h_{j}^{q} \right) \right) /Z_j$$表示段落序列中每个token对应的权重。$$Z_j=\sum_{k=1}^{\left\| p \right\|}{\exp \left( score\left( h_{k}^{p},h_{j}^{q} \right) \right)}$$。$$W_q$$与$$b_q$$为可学习的参数。


**decoder层**为单层单向LSTM，其隐状态为

$$
s_t=f_{LSTM}\left( s_{t-1},\left[ y_{t-1};c_{t-1}^{p};c_{t-1}^{q} \right] \right) 
$$

其中$$c_{t-1}^{p}$$与$$c_{t-1}^{q}$$分别表示段落的语义编码和问题的语义编码，其计算方式与$$c_t$$类似。

**接下来的预测词和复制机制的部分与seq2seq相同。**


## 训练与推断
上述两个模型的目标是在给定可回答问题$$q$$和包含答案$$a$$的段落$$p$$的条件下，使生成的无法回答的问题的可能性尽可能大。具体目标函数为

$$
\mathcal{L}=-\sum_{\left( \tilde{q},q,p,a \right) \in \mathcal{D}}{\log P\left( \tilde{q}|q,p,a;\theta \right)}
$$

其中$$\mathcal{D}$$表示训练语料，$$\theta$$表示模型中所有的可学习参数。

推断时，$$\underset{q'}{argmax}P\left( q'\|q,p,a \right)$$表示生成的最有可能的无法回答的问题序列为$$q'$$。同时，为了避免遍历所有可能的输出，同时保证生成的多样性，在生成时使用了Beam search。

## 实验结果
**生成质量**的评估结果如下图所示：

![png](/assets/images/nlp/mrc/mrc-da-04.png)

可看出pair2seq相比seq2seq的生成质量更好的无法回答类问题。且输入的可回答问题对生成质量起着重要作用。

下图给出了在现有模型基础上通过增强无法回答问题的数量，EM与F1两个性能指标都有所提高。

![png](/assets/images/nlp/mrc/mrc-da-05.png)

下图展示了对生成的无法回答的问题从不可回答性、相关性及可读性三个方面的人工评价结果。

![png](/assets/images/nlp/mrc/mrc-da-06.png)

其中TFIDF的方法为baseline，其使用输入的可回答问题来检索对其他文章的类似问题作为输出。这种方式检索出的问题大多是不可回答且可读的，但和输入的段落与问题十分不相关。

SQuAD2.0数据集中原有的不可回答的问题有六类，下图统计了生成的不可回答问题的类型分布情况。

![png](/assets/images/nlp/mrc/mrc-da-07.png)

其中自动生成的不可回答问题主要由加入否定词和实体替换两类组成，相比原有的问题类型分布有点单一。

下图对比了不同的生成无法回答问题的方法的效果。

![png](/assets/images/nlp/mrc/mrc-da-08.png)

有使用TF-IDF来检索其他文档的问题的方法；在可回答问题中加入反义词、替换实体等基于规则的方法，对比各种方法可看出pair2seq的性能最好。

同时为了检验增强数据的规模对阅读理解系统的影响，通过beam search方法为每个输入生成多个不可回答的问题。

![png](/assets/images/nlp/mrc/mrc-da-09.png)

在BERT-base和BERT-large模型上进行实验，结果如下表所示，在较小的BERT-base模型上，扩大增强数据规模能够取得进一步的提升，但数据规模对BERT-large模型几乎没有影响。

## 参考
- [Learning to Ask Unanswerable Questions for Machine Reading Comprehension](https://arxiv.org/abs/1906.06045)

- [Resources of ACL2019 paper "Learning to Ask Unanswerable Questions for Machine Reading Comprehension"](https://github.com/haichao592/UnAnsQ)

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

- [ACL 2019 机器阅读理解中不可答问题的数据增广](https://www.zhuanzhi.ai/document/3a768850bda2a49fdcddfe980fae35c6)

- [论文梳理：问题生成(QG)与答案生成(QA)的结合](http://www.shuang0420.com/2018/07/08/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90(QG)%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90(QA)%E7%9A%84%E7%BB%93%E5%90%88/)
