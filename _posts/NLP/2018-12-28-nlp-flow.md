---
layout: post
title: 中文自然语言处理的一般流程
categories: NLP
description: 中文自然语言处理的一般流程
---

# 概述
首先从分析对象与分析内容两个维度来宏观的了解NLP所包含的技术知识点。(仅做参考)

**分析对象**
- 词汇级
    - 语法分析
        - 中文分词
        - 词性标注
        - 命名实体识别
        - 新词发现
    - 语义表示
        - 语义表示
        - 语义消歧
    - 语义关系
        - 语义关系建模
        - 语义关系抽取
        - 语义关系计算

- 句子级
    - 语句变换
        - 近义词替换
        - 语义归一化
        - 省略/纠错
    - 语句解析
        - 句法结构分析
        - 依存关系分析
    - 语句表示
        - 语义表示
        - 文本分类
    - 语句生成
        - 规则模板
        - 知识图谱
        - 机器翻译

- 段落/篇章级
    - 单文本分析
        - 文本主题
        - 文本分类
        - 文本结构分析
        - 文本语义分析
    - 多文本分析
        - 文本主题
        - 文本分类/聚类


**分析内容**
- 词法分析
    - 中文分词
    - 词性标注
    - 命名实体识别
    - 新词发现

- 语法分析
    - 句子级
        - 句法结构分析
        - 依存关系分析
    - 段落/篇章级
        - 文档结构分析

- 语义分析
    - 词汇级
        - 语义表示
        - 语义消歧
    - 句子级
        - 语义表示
        - 文本分类
        - 意图识别
        - 情感分析
    - 篇章/段落级
        - 语义表示
        - 文本主题
        - 文本分类/聚类
        - 情感分析

- 语用分析
    - 内容分析
        - 语境分析
        - 句意理解
    - 内容生成
        - 规则匹配
        - 知识推理
        - 机器翻译
        
中文自然语言处理的过程与机器学习过程大体一致，但有很多细节上的不同点。下面介绍其基本过程。

# 获取语料
语料，即语言材料。我们将一个文本集合称为语料库(corpus)。按语料来源，可将语料分为以下两种

1. 已有语料
    很多组织、公司等都会积累大量的纸质或电子文本资料。将纸质的文本全部电子化即可作为我们的语料。
    
2. 网上下载、爬取语料
    当没有数据时，可以选择获取国内外标准开放数据集，如国内的搜狗语料、人民日报语料。也可以通过爬虫自己爬取一些数据

# 语料预处理
在一个完整的中文自然语言处理的工程应用中，语料预处理大概会占到50%-70%的工作量。下面主要从语料清洗、分词、词性标注、去停用词四个方面介绍语料预处理工作。

1. 语料清洗
    语料清洗，就是在语料中，保留感兴趣的内容，清除不感兴趣的内容。对原始文本，提取标题、摘要、正文等信息；对爬取的网页内容，去除广告、标签、HTML、JS等。

2. 分词
    常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。
    
    **当前中文分词算法的主要难点有歧义识别和新词识别**。比如：“羽毛球拍卖完了”，这个可以切分成”羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。

3. 词性标注
    词性标注就是给每个词打上词类的标签，如形容词、名词、动词等，这样可以让文本在后面的处理中融入更多的语言信息。不过对某些自然语言处理的任务来说，词性标注不是必须的。如常见的文本分类就无需关心词性问题，但类似情感分析、知识推理却是需要的。
    
    常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。

4. 去停用词
    停用词一般指对文本特征没有任何贡献作用的字词，如标点符号、人称等词。所以在一般的文本处理中，分词后接下来就是去停用词。停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。
 
# 特征表示
做完语料预处理后，接下来需考虑如何把处理后的语料表示成计算机能够处理的类型。常用的表示方式有词袋和词向量。

词袋(Bag Of Word,BOW)，即不考虑词在句子中的顺序，基于统计的方式(统计词频/是否出现)进行表示，其维度为词表大小。TF-IDF/One-Hot都是词袋的一个经典用法。

词向量属于分布式表示，相比高维且稀疏的词袋，其不但具有低维稠密的特征，还可以较好的表示不同词间的相似性。典型的模型有跳字模型(Skip-Gram)、连续词袋模型(Continuous Bag Of Words,CBOW)、Doc2Vec、WordRank和FastText等。

# 特征选择
上述构造出的文本特征，在实际问题中还要进一步选择合适的、表达能力强的特征。但在特征选择的过程中，将会丢失部分语义信息。因此特征选择是一个很有挑战的过程。常见的特征选择的方法有DF、 MI、 IG、 CHI、WLLR、WFO 六种。

# 模型训练 
在特征选完后，针对不同的应用需求，我们采用不同的模型。传统的机器学习模型，如KNN/SVM/NB/GBDT/K-means等；深度学习模型，如CNN/RNN/Transformer/FastText等。 下面是模型训练时需要的注意点。

1. 注意过拟合、欠拟合问题，不断提高模型的泛化能力
    过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。
    
    欠拟合：模型不能够很好地拟合数据，表现在模型过于简单。

2. 对于神经网络，注意梯度消失和梯度爆炸问题

# 评价指标 
训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。

1. 错误率、精度、准确率、精确度、召回率、F1

2. ROC 曲线、AUC 曲线

# 模型上线应用
目前主流的应用方式就是提供服务或者将模型持久化。

# 模型重构     
随着业务的变化，可能需要对模型做一定的重构，需对上面提到的各个步骤进行调整，重新训练模型并再上线。

# 参考文献
- [中文自然语言处理](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/pdf/chinese-nlp.pdf)
- 周志华 --- 机器学习
- 李航 --- 统计学习方法
- 伊恩·古德费洛 --- 深度学习